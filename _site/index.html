<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" dir="ltr" lang="en-US">
  <head profile="http://gmpg.org/xfn/11">
    <title>Home &mdash; In Valid Logic</title>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
    <meta name="description" content="Endlessly expanding technology" />
    <meta name="author" content="Ken Robertson" />
    <meta name="generator" content="jekyll" />

    <link href="/css/site.css" rel="stylesheet" type="text/css" media="screen" />
    <link href="/css/pygments/trac.css" rel="stylesheet" type="text/css" media="screen" />

    <link href="/atom.xml" rel="alternate" title="Ken Robertson's Atom Feed" type="application/atom+xml" />
    <link href="/rss.xml" rel="alternate" title="Ken Robertson's RSS Feed" type="application/rss+xml" />

    <link rel='index' title='In Valid Logic' href='http://invalidlogic.com/' />
  </head>

<body>
  <div id="container" class="group">

    <h1><a href="http://invalidlogic.com">In Valid Logic</a></h1>
    <div id="bubble"><p>Endlessly expanding technology</p></div>

    <div id="content" class="group">
      
  <h2><a href="/2011/02/16/our-pain-points-with-ec2/">Our pain points with EC2 and how our moved solved them</a></h2>

  <div class="main">
    <p>As I mentioned in my last post, we made the move off of EC2 in December to our own cluster of machines.</p>

<p>While EC2 had served us very well and helped get our systems to the
place they are today, we started reaching a point where we were
running into three main bottlenecks with the EC2 ecosystem and we couldn't find any way around them.</p>

<p>Will start off by saying the AWS is an awesome service and we couldn't
have grown to the level we are at today without them.  Amazon should
be a easy contender for any service just starting out.  Some of the
services they provide, especially things like Elastic Load Balancer
(ELB) and Elastic Block Storage (EBS) are services that are simple to
start using, help tremendously as you grow, and simply aren't common
with traditional VPS hosting.</p>

<p>However, here are some of the bottlenecks we faced and how we've
solved them in our new setup.</p>

<h3>IO Performance</h3>

<p>Anyone who is doing anything intensive on EC2 will tell you that IO
performance is horrendous.  We were battling EBS constantly on our
database servers and had serious questions how we could continue to
operate yet alone scale our DBs while in the cloud.  We met with AWS engineers to review our configurations, but
were already doing all the best practices.  We had tuned MySQL
configuration as much
as we could, had the "high-memory" instances and higher IO priority
and newer CPUs,
and were running 4 EBS volumes in RAID10.</p>

<p>They tried to push Amazon RDS, however RDS is simply EC2 and EBS with
the same best practices.  While it might be slightly better, it wasn't going to
totally solve our problems.</p>

<p>The biggest issue was IO latency.  We had database servers where it
was normal for them to average 20-30% IO wait.  We frequently had
spikes upwards of 60% to sometimes 90%.</p>

<p>And it wasn't always disk performance, but IO as a whole.  We would at times provision a
new database server that would simply struggle to stay fully
replicated with no query load on it.  Either struggling with IO
latency, or sometimes even keeping a connection up to the master.</p>

<p>There is unfortunately no solution.  Everything in AWS is a shared
resource.  With that comes variable performance and bad spikes due to
hot-spots.  It is difficult to predict or ensure consistent
expectations.</p>

<h3>Support</h3>

<p>At one point, we encountered an issue where we were seeing extreme IO
latency on our EBS volumes on our master MySQL DB server.  One of our
other ops guys was up for several hours in the middle of the night,
along with our CTO and Director of Product.  Unfortunately it was an
EBS issue throughout the entire zone and we had a large footprint in
that zone, including our master database server.</p>

<p>The biggest thing we took issue with was that it was almost 3 hours
before it actually made it to Amazon's status dashboard.</p>

<p>After that incident, we started looking at Amazon's Premium support,
however the support packages are really quite weak when you dig into
them.</p>

<p>First of all, Premium is a straight 20% added to your bill.  You can't
select it per instance or anything.  We didn't care about premium
support on our staging systems, so we'd have to move those to another
account if we signed up.</p>

<p>But the big thing... you get 24/7 phone support and a 1 hour
acknowledgement SLA.  Meaning they'll confirm you are having an issue
within 1 hour.  However, resolution is another thing.  Because
everything in AWS are shared resources, they can't offer any sort of
priority resolution at all.</p>

<p>So that EBS latency issue?  It took 3 hours to make their status
dashboard, and a total of 11 hour from when we noticed it to them
posting it was totally resolved.</p>

<p>Paying for Premium support wouldn't have helped much.  It would still
be fixed for us at the same time.  At telling the CTO/Director "AWS
has acknowledged it, it'll be fixed when its fixed" isn't the type of
message we like to deliver.</p>

<h3>Scaling Costs</h3>

<p>Amazon is great for services just starting out.  It is simple pay-as-you-go, you get availability of a lot of nice functionality with S3,
Elastic Load Balancer, and others, and it is quick to get bigger instances
as you grow and need more.</p>

<p>However there comes a threshold where the costs start growing more
quickly than your scale.</p>

<p>Want support?  Bam, extra 10%-20%.</p>

<p>IO bottlenecks?  Amazon's answer is to use bigger instances (higher IO
priority) and spread the load across multiple instances.  Both of those mean
more money.</p>

<p>Once you start reaching bottlenecks, the only answer within AWS is to
work around them, and that usually means more costs.</p>

<p>Additionally, since it is so easy to provision new systems, it is easy
to get carried away without realizing how much the changes are costing you.</p>

<h3>Insight</h3>

<p>This is actually a bottleneck we didn't really realize until after we
moved.  It is amazing how little insight you have into your
architecture until you have full control over it.</p>

<p>For instance, on the new setup we had nice zone separation between
systems and were seeing massive spikes in concurrent connections
through the firewall, 3x more than we expected.  They all ended up
being DNS traffic.  Since it was UDP, the "connection" count was
skewed, but we hadn't realized how much DNS traffic we generated.  In
AWS, we couldn't monitor low level traffic metrics in the same way,
track our aggregate DNS traffic, or anything.</p>

<h3>Our Solutions</h3>

<p>Our solutions to all of these were actually quite simple and are a
nice blend between old school architecture and modern pragmatism.</p>

<p>Nowadays, you start to hear all this stuff about "private clouds".
First of all, drop the bullshit.  "Cloud" is slapped on anything it
can be these days.  There is nothing new to the concept of "private clouds".</p>

<p>All it is is a virtualization cluster on equipment dedicated to you.
Its been around for 10+ years.  Clouds caught on because they were
simple to get started with and great at the small scale.  Now its just
a marketing buzzword.</p>

<p>Our approach was simple.  We separated our environment out into a
hybrid of dedicated systems for some things, and a large SAN backed
virtualization cluster for everything else.</p>

<h4>Databases</h4>

<p>Dedicated hardware is the perfect solution for IO intensive
workloads.  With this, we could actually specify what we wanted.
Types of disks, RAID configuration, capacity, etc.  We could go as
simple as RAID 10, up to SSD or even FusionIO.</p>

<p>We moved all our MySQL and MongoDB systems to dedicated hardware, each
built to their own needs and planned ahead for a certain timeline.  We
then looked at several growth metrics.  For instance, MySQL would
likely need more CPU or disk capacity in the future.  MongoDB would
likely need more RAM in the future.</p>

<h4>Virtualization</h4>

<p>Almost everything else was virtualization.  We got several very big,
bulky virtualization systems.  12 cores, 24 threads, 128gb RAM, ohh
yeah.  However we slice and dice it is up to us.  We have everything
configured for full failover of the base chassis.</p>

<p>At the SAN level, we left that up to the guys at Network Redux.  They
monitor the workload levels and plan for it accordingly.  The IO
doesn't scale infinitely, but rather we try to maintain an average
expectation so that growth is predictable.</p>

<h4>Support</h4>

<p>Support is probably one the biggest and most dramatic changes.  AWS is
a complete black box.  Now, we have two people dedicated to our
account, as well as a 24 support desk we can call and have a tech pull
up all the details on our account.  We have the cellphone numbers of
the people dedicated to our account.  We have yet to call them at 2am,
however having it is more than we ever had with Amazon.</p>

<p>We now have people proactively working to help us.  If IO load is
growing on the SAN, they reach out to us ahead of time, before it
becomes a problem on our end.</p>

<p>If a hardware node throws an alert, they get paged at the same time
and will hop online to help us to investigate.</p>

<p>This is probably one of the biggest pros and our new provider, <a href="http://networkredux.com/">Network Redux</a> has been truly awesome in that regard.  They are proactive, we deal with the same people over time so they are familiar with our deployment, our change history, etc.</p>

<h4>Costs</h4>

<p>Costs though... surely all this raw hardware goodness, scalable IO,
and choice in your deployment costs a whole lot extra, doesn't it?
Not so.  You might actually be surprised how much cheaper it is.  I
can't give numbers, but I will say it was <em>significant</em> and after the
move one of the main bullet points added by the CFO was <em>how much</em>
cheaper it was.</p>

<p>To make the CFO even happier, we had planned ahead for growth, where
we could handle spikes and add some additional systems without extra
costs, and had solid figured on how much the costs would increase.
"When we need more capacity here, the next step is X, and the cost
will be $Y."  As more time goes on, we're getting better at predicting
when the growth will be needed, and showing the benefits.</p>

<h3>Conclusions</h3>

<p>Overall, Amazon is great service.  Don't get me wrong.  It is truly
excellent for companies/services just starting out and has an awesome
growth path for scaling early on.</p>

<p>However nothing scales infinitely.  Not your applications, and not the
benefits of a service like AWS.</p>

<p>Once you each a certain threshold, you benefit more from full control
over your environment and you get to the point where it is actually
cheaper than the cookie cutter services AWS provides.  Where that
threshold is depends on each case.  But it is important to know where
it is, even if you aren't there yet.</p>

<p>For us, we improved our performance several times over, have more
control than ever, and did it all while actually saving money.</p>

  </div>

  <div class="meta group">
    <div class="signature">
      <p>Wednesday, February 16, 2011</p>
    </div>	
    <div class="tags">
      &nbsp;
    </div>
  </div>

  <h2><a href="/2011/02/11/how-we-did-a-datacenter-migration-with-no-downtime/">How we did a datacenter migration with no downtime</a></h2>

  <div class="main">
    <p>Starting in October, we at Involver decided to take on a project migrate our entire infrasture off of Amazon EC2.  AWS had served us well, however there comes a point where the limitations of some of their services were becoming a major bottleneck.  The bottlenecks and how we've resolved them in our migration is a big enough topic for a follow up post, but we recognized that we needed more control over our environment and the specifications of all the systems.</p>

<p><strong>Update:</strong> Have posted the follow up on the <a href="/2011/02/16/our-pain-points-with-ec2/">bottlenecks we were facing EC2</a>.</p>

<p>After nearly a month of evaluating providers, doing benchmarks on test systems, and milling over all the details, we picked our provider.  We then spent another month refactoring various systems, re-evaluating all machine roles, and system configurations.  Since we were no longer constrained by EC2 instance types, we started looking at how many CPUs and how much RAM is really best suited for each system.</p>

<p>For our hosting provider, we ended up going with <a href="http://networkredux.com">Network Redux</a>.  Of all the others, both big names you'd recognized and some smaller ones you wouldn't, they proved the best fit.  Go to any provider and they can all get you the same server.  That is easy.  Its the other services, the skills of their staff, and quality of the total, big picture architecture that matters.  With Network Redux, a lot of it came down to the fact they they worked like us.  They hired top-notch people, they moved quickly on things, and had a short decision tree.  Case in point, when the President is on your sales call and can say you'll have a test system with set specs in two days, vs the sales person needing to pull out an org chart and "get back to you."</p>

<p>After our month of refactoring, we finally flipped the switch the night of December 23rd.  We did an entire datacenter migration with no user downtime.  Even more, we didn't even have all of our production systems until that morning (thanks Dell! &lt;/sarcasm&gt;).</p>

<p>So how did we pull it off?</p>

<p>It boils down to an excellent team, delicate planning, and an awesome toolset.  Our operations team is only 3 people, but we manage what is now 8 environments where our code lives and about 100 systems.</p>

<p>Everything in our environment is fully configured with chef.  With the migration, we refactored a lot of our recipes put a strong emphasis on everything in chef.  Each system was provisioned, cooked and recipes tweaked, then destroyed, reprovisioned, and recooked.  Repeated until it was ready to go right after cooking.</p>

<h3>MySQL</h3>

<p>Our production database servers were the first to get set up on the production side.  We'd already validated all our chef recipes for both our MySQL and our MongoDB clusters on our staging environment beforehand.  With MySQL, we run a master-master setup with multiple read-only slaves.  We cooked each of the boxes and configured all of the slaves to point to what would be the new master.  We made a change to ensure that the new master and the EC2 master had different auto-increment offset and we also ensured log_slave_updates was enabled.  We then imported a backup of our production database onto the new master.  It automatically replicated out to the passive master and read-only slaves.  We then set it up so our new master would replicate from our EC2 master.</p>

<p>With this configuration, writes would be coming in on our EC2 master, get replicated to the new master, and since it has log_slave_updates on, those changes would then replicate our to the other database servers.  This also enabled us to switch over the site and easily allow writes to start coming in to the new environment.  If there were some lagging requests to the old site, they would replicate to the new environment without any collisions.  We also documented the position of the new master when the switch happened, so if we needed to revert, we could potentially set the EC2 master to replicate from the other master.</p>

<h3>MongoDB</h3>

<p>The MongoDB databases were migrated in a different fashion.  We were moving from a single replica set configuration to a sharded setup with two replica sets.  Because of this, we could do the same early replication process we did with MySQL.  Mongo is primarily used for our analytics data, so with it, there is a primary collection that is most important and then others used for the calculations.  We cooked the database servers ahead of time and got the sharding configuration and everything fully setup before hand.  Then the data migration was scripted out so that with a quick command, we would pull the most important data over first by exporting the data, transferring it, then importing it.  This way, it was migrated and sharded at the same time and fully automated.</p>

<h3>Our Application</h3>

<p>The morning of the 23rd, we came in, having just had the bulk of our production virtualization cluster setup the night before (thanks again for the delay Dell).  This is where the awesomeness of chef and our team truly shined.  We set to work and provisioned all of the production systems, cooked them, set them up in load balancing, verified security rules, and completed some of the final code tweaks and testing.  We didn't even deploy our final monitoring systems until that morning.   We did a few test deploys, verified all the parts of the app would come up, and that monitoring on them was accurate.</p>

<h3>The Switch</h3>

<p>Finally, around 10:45pm, we made the switch.  The actual cut off boiled down to just a DNS change.  TTLs had been changed to 60 seconds weeks earlier.  We just repointed the necessary hosts and within 60 seconds, we saw the majority of traffic cease on EC2 and come in on our new systems.</p>

<p>Right after we made the DNS change, we also removed all of our old app servers from ELB (Elastic Load Balancer) except for two.  We turned nginx off on those two and turned on <a href="/2010/02/12/migrating-datacenters-how-to-forward-traffic">rinetd</a>, which was already configured to forward any traffic to port 80 or 443 to our new environment.</p>

<p>The benefits of the migration were almost instantly visible.  Noah, our CTO, was in the office with us to do acceptance testing right afterward.  We made the switch, verified DNS had updated, and when he loaded the first page, his first reaction was "Holy shit... this is so much faster!"  That there was all the validation we needed for the 2 months of work we had put in.</p>

<p>Most impressive was looking back at data in New Relic from before and after the migration.  It was quite surprising how drastic the improvement was.</p>

<div style="text-align: center"><img src="http://invalidlogic-blog.s3.amazonaws.com/redux-switch.jpg" width="647" height="264" /></div>


<p>And its gotten even better.  Having to no longer fight fires with AWS anymore, we've actually made tons of other enhancements.</p>

  </div>

  <div class="meta group">
    <div class="signature">
      <p>Friday, February 11, 2011</p>
    </div>	
    <div class="tags">
      &nbsp;
    </div>
  </div>

  <h2><a href="/2011/02/07/how-a-deprecated-method-can-take-down-a-process/">How a deprecated method can take down a process</a></h2>

  <div class="main">
    <p>At Involver, we have a pretty active background processing tier that handles a variety of things from long running tasks we don't want in the web app, pulling in content from external sources, and simple recurring maintenance tasks.</p>

<p>Previously, we ran a single worker thread per JVM, but that setup lead to a low worker per system density since it sucked up a lot of memory.  A few months ago, we came up with a way to run multiple workers within a single JVM and have it nicely thread safe by using a small embeddable JRuby VM started off the main process.  This worked excellent and doubled the number of workers we could run per machine, since they could share heap space.</p>

<p>But then we recently began experiencing an issue where all the threads would just halt processing.  No errors, seemingly nothing wrong.  Heap usage was fine and a thread dump showed them seemingly ok, but they just sat.  We tried attaching a profiled, however it ended up just output garbage when we tried to have to dump its data after it got stuck.</p>

<p>After 2 days of investigation, finally managed to tracked the cause down to the fact that a change a few days before had caused us to start running over a function that used a deprecated method call, and the change meant we were hitting that line 500-1000 times per minute.</p>

<div style="float:right; margin-top: 5px; margin-left: 10px; margin-bottom: 5px"><img src="http://invalidlogic-blog.s3.amazonaws.com/gru-light-bulb.jpg" width="400" height="231" title="not a light bulb, but still very shiny" /></div>


<p>At first, we suspected maybe JRuby had some sort of thread concurrency issue with calling deprecated methods, but that made no sense.  All calling a deprecated method does is write to the console!</p>

<p>As Gru in Despicable Me would say... "Light bulb!"</p>

<p>When running a single worker in console, we saw the warnings, but when running the multiple workers per JVM, we saw none.  The rabbit hole gets deeper and fortunately lead us to the solution and were able to confirm the issue and resolution.</p>

<p>The way we spawned the child threads, we found stdout/stderror was not attached to anything.  They make use of buffered IO though.  There is a fixed buffer size, and if you try writing more than that, it will block on writing to it until the buffer is read from and space is freed.  Since nothing was reading from it though, the threads would simply block indefinitely on writing out the deprecated method calls.</p>

<p>So we set up a test.  We created the following class on one of our boxes and spawned up the multi-worker process.</p>

<div class="highlight"><pre><code class="ruby"><span class="k">class</span> <span class="nc">Awesome</span>
  <span class="k">def</span> <span class="nc">self</span><span class="o">.</span><span class="nf">kill_me_softly</span>
    <span class="kp">loop</span> <span class="k">do</span>
      <span class="nb">puts</span> <span class="s2">&quot;a&quot;</span> <span class="o">*</span> <span class="mi">1024</span>
      <span class="no">Rails</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span> <span class="s1">&#39;Alive!&#39;</span>
    <span class="k">end</span>
  <span class="k">end</span>
<span class="k">end</span>
</code></pre>
</div>


<p>It came up, but would get hung in less than a second.  Simply do no more.  One threads would lock up independently, one by one.  So the whole JVM wasn't frozen, just that child thread.  Hence why they all died at different times, but very close together.</p>

<p>Then to confirm even more and fix it, we changed the multi-worker process to reopen the stdio handlers on child threads:</p>

<div class="highlight"><pre><code class="ruby"><span class="no">STDIN</span><span class="o">.</span><span class="n">reopen</span> <span class="s2">&quot;/dev/null&quot;</span>
<span class="no">STDOUT</span><span class="o">.</span><span class="n">reopen</span> <span class="s2">&quot;/dev/null&quot;</span>
<span class="no">STDERR</span><span class="o">.</span><span class="n">reopen</span> <span class="s2">&quot;/dev/null&quot;</span>
</code></pre>
</div>


<p>Spawn one back up, and it begins streaming to no end, no lock ups, no problems.  Success!</p>

<p>In the end, we changed the function to no longer use the deprecated method, and had the multi-worker process redirect stdio for child processes to an actual file, so that way we can actually log any valuables messages we might be otherwise missing.  But that is hopefully the first and only time something as off as stdio buffering takes down a process.</p>

  </div>

  <div class="meta group">
    <div class="signature">
      <p>Monday, February 07, 2011</p>
    </div>	
    <div class="tags">
      &nbsp;
    </div>
  </div>

  <h2><a href="/2011/01/19/love-your-dependency-management/">Love your dependency management</a></h2>

  <div class="main">
    <p>Dependency management is a pain in the ass, but it doesn't have to be.</p>

<p>Last month we went through a full migration of all of our systems to a new provider.  This is truly awesome to do when you have all your systems spec'd out and managed with chef, however it can also highlight those areas you might have been lazy or where you didn't enforce your chef specs enough.  We managed to catch a few places where a smaller satellite app had newer system gems installed and not documented in its readme or in our chef specs.</p>

<p>Managing gems is one of those areas that you find varying opinions, but I've definitely fallen in love with <a href="http://gembundler.com">bundler</a> over the past year.  I use it on all my personal projects, but it becomes even more valuable when used within a team of developers.  Need to change a gem?  Just do it and notify everyone to run bundle install.  Often times, bundler will actually tell them they need to update if it sees changes.  Time to deploy your changes?  That is all you do... deploy them!</p>

<p>Before, we had a mixture of vendored gems and system gems depending on whether some were using C extensions or needed to have system binaries.  But by moving these applications to use bundler, the only gem chef needs to install is bundler itself.</p>

<p>The easing of the process for both sides is awesome... both for the devs and for operations.</p>

<p>Tired of your own gem deploy or dependency management woes?  Take a look... wouldn't hurt.</p>

  </div>

  <div class="meta group">
    <div class="signature">
      <p>Wednesday, January 19, 2011</p>
    </div>	
    <div class="tags">
      &nbsp;
    </div>
  </div>

  <h2><a href="/2010/12/08/progress-through-iterating/">Progress through iterating</a></h2>

  <div class="main">
    <p>It was a little over a year ago that I launched the public beta of Trunks, and when talking with a friend was looking back at all the revisions it has gone though.  Although its only been live a year, I've actually been working on the project for 3 years now and I'm on the 4th major revision of its codebase.</p>

<p>The projects that are most fulfilling long-term are those that you have a vested interest in.  Often times, they're beyond purely financial.  I have never gauged Trunk's success on its profits, rather it is an idea that I myself find value in, and believe others do as well.  That is why I've stuck with it over time.</p>

<p>Each major revision has addressed problem areas of the previous and has gotten incrementally better and more mature.  When looking back, can clearly see the problem points of the previous version and how they were addressed.  It is important to look back to see the path you've traveled.  Be aware of the mistake you've already addressed so as to not re-introduce them.  And if nothing else, feel proud of the investment you've made in the project and you own growth.</p>

<p>I had basically forgotten I'd been working on Trunks for 3 years.  After being reminded, was looking at it and thinking damn, its come a long way.</p>

  </div>

  <div class="meta group">
    <div class="signature">
      <p>Wednesday, December 08, 2010</p>
    </div>	
    <div class="tags">
      &nbsp;
    </div>
  </div>

  <h2><a href="/2010/10/25/tool-enlightenment-part-2/">Tool Enlightenment, Part 2</a></h2>

  <div class="main">
    <p>I started typing a follow up post about my experiences with vim, but then as I was adding a line about not throwing gasoline on the holy war that I was doing exactly that.  I was creating the stereotypical holy war inflamatory post.</p>

<p>My main point in the previous post was about finding the tools that worked best for you, and ignoring the FUD from everyone else.</p>

<p>So with that, I spent a week with vim, watched several screencasts, pulled in several plugins, and gave it a solid week of not using emacs and resisting TextMate.  At the end of that week, I've decided I'm more at home in emacs.</p>

<p>The reasons are basically mute, especially since I'm trying to push for people to find out for themselves.  It is counter intuitive to give a now slanted opinion while telling you to ignore it.</p>

  </div>

  <div class="meta group">
    <div class="signature">
      <p>Monday, October 25, 2010</p>
    </div>	
    <div class="tags">
      &nbsp;
    </div>
  </div>

  <h2><a href="/2010/10/14/tool-enlightenment/">Tool Enlightenment</a></h2>

  <div class="main">
    <p>Recently, I decided to take a journey down the path of enlightenment.</p>

<p>One of the things I found so refreshing about Ruby/Rails when I first started using it was the simplicity of the tooling.  Just use a text editor, no monolithic IDE with large memory footprints, slow load times, and a panache for crashing.</p>

<p>Visual Studio certainly is a great tool, and provides some powerful tools to .NET.  But I doubt anyone would disagree with me that Visual Studio often feels like a 500lb gorilla. Especially when you add in the plugins so many use, like CodeRush or ReSharper.  Simply opening TextMate was rather refreshing when coming from that kind of environment.</p>

<p>A few weeks ago, I had the chance to go to <a href="http://gogaruco.com">GoGaRuCo</a> and they had a workflow panel where they got some of the well known developers up on the stage, and discussed some of the tools and arrangements they use.  It was a little superficial, where it was "I have an editor with 3 panes and 2 terminals open".  But there were some interesting observations.  First of all, everyone on the panel was using either Emacs or Vim.  After the talk, they polled the audience what they use.  The overwhelming majority of the audience was using either RubyMine or TextMate.</p>

<p>The surface argument you could make is that all the "hackers" are using Emacs/Vim and aren't representative of the population.  Sure, I looked at it that way too.  But a few things started to peak my interest, particularly when I'd talk to an Emacs or Vim user.  You will almost always hear them say they've been using it for 10 years, 15 years, or sometimes even longer.</p>

<p>Think about your toolset today and your toolset 10 years ago.  Is there <i>anything</i> that you're still using?  If so many people will stick with the same tool for 10 years, spanning technology bubbles, and crossing over to new languages, there must be something that tool is doing right.  These are tools whose users invest in them.  They're like marriages.</p>

<p>So my journey to enlightenment has begun with looking at Emacs, however I'm not just playing eenie-meenie-miney-moe. Both Emacs and Vim have large followings are both excellent choices. The way I see it, I can't chose which direction to go without looking both ways.  So, I'm trying both for 1-2 weeks each, play around with building my .emacs and vimrc, and see where it leads.</p>

<p>I've been using Emacs for almost 2 weeks now, and find myself opening stuff up in TextMate less and less.  Next week, I plan to focus more on Vim and try it out.</p>

<p>But I think the true enlightenment that can be found is... the elite tools are not what the "hacker" portion of a community use.  Use what fits your style best, understand why you've chosen your tools, and recognize those same decisions in the tools used by those around you.</p>

  </div>

  <div class="meta group">
    <div class="signature">
      <p>Thursday, October 14, 2010</p>
    </div>	
    <div class="tags">
      &nbsp;
    </div>
  </div>

  <h2><a href="/2010/10/08/involver-team-and-matz/">Involver Team and Matz</a></h2>

  <div class="main">
    <p>Yukihiro "Matz" Matsumoto was speaking yesterday at the <a href="http://www.twilio.com/">Twilio</a> offices in San Francisco on what is coming in Ruby 2.0.  Was a great time and was a pleasure to meet Matz in person.  I love to meet people who have changed the industry, but still remain humble.  He was very polite, thanking us for coming, shaking hands, meanwhile everyone was thanking him for Ruby.</p>

<p>One of the great question after the presentation was someone asking him what it is like having, basically, changed development and how it felt when he first realized it was catching on.  Matz said that time was around when the first picaxe book came out.  It was having someone other than himself writing an English book about Ruby and it selling well.  But his general answer to what it was like was "its nice".</p>

<p><img src="http://invalidlogic-blog.s3.amazonaws.com/100_0098_2.jpg" alt="" title="100_0098_2" width="640" height="480" class="aligncenter size-full wp-image-609" /></p>

<p>The Involver team had the opportunity to take a bit of a team photo with him.  This represents about a quarter of the total Involver Engineering team.  Here we have <a href="http://twitter.com/zedlander">@zedlander</a>, Jaime our System Overlord, myself, Nina our QA queen, and <a href="http://twitter.com/zquestz">@zquestz</a> (aka Josh) who just started this week.  <a href="http://twitter.com/nolamn">@nolman</a> was also there, but had to take off right after the talk.</p>

<p>Ruby 2.0 will have a number of small but very nice language improvements, such as keyword arguments (similar to what .NET 4.0 added), as well as the ability to namespace things, including monkey patching that is scoped to only the namespace.  That there is something that seems like it will bring a lot of power.  Use two libraries that use two different versions of a gem?  What if you could load them in separate namepaces, each utilizing the separate gem versions.  Perhaps you could get really crazy and have a Rails 2 and Rails 3 app running together using separate namespaces.  I don't know why anyone would do that, but you could, and that I like.</p>

<p>Ruby 2.0 will see a large emphasis on embedded and an alternative VM/interpretter for more targeted implementations.  I would love it if we could have a setup similar to the Arudio or Neturdio that ran Ruby.  Matz was talking about bringing Ruby to places like digital appliances, more RTOS environments focusing on latency instead of throughput.  Or having Ruby implementations for the cloud that utilized a smaller subset of the language to run on more targeted hardware.  Distributed processing where perhaps computational power is more important than IO or what not.</p>

<p>My first thought... in a few years, could my DVR be running a Rails web interface?</p>

  </div>

  <div class="meta group">
    <div class="signature">
      <p>Friday, October 08, 2010</p>
    </div>	
    <div class="tags">
      &nbsp;
    </div>
  </div>


    </div>

    <div id="sidebar">
      <h3>About</h3>
      <div class="textwidget">
        This is the personal blog of Ken Robertson.  I'm a developer at <a href="http://involver.com/">Involver</a> working in Ruby/Rails.  In a former life, I worked in .NET for many years and since moved on.<br/>
        <br/>
        Now, I get to make Rails scale, cook all day long with chef, and experiment with building highly concurrent systems.<br/>
        <br/>
        In my spare time, I like to build things.  I like to experiment with different technologies to find how they best fit onto my utility belt.<br/>
        <br/>
        I sometimes blog.  You'll find those things here.<br/>
        <br/>
        I sometimes code.  You'll find those on <a href="http://github.com/krobertson">Github</a>.<br/>
      </div>

      <h3>Search</h3>
      <form method="get" action="http://www.google.com/search">
        <div>
          <label class="screen-reader-text" for="s">Search for:</label>
          <input type="text" id="s" name="q" value="" />
          <input type="hidden" name="q" value="site:invalidlogic.com" />
          <input type="submit" id="searchsubmit" value="Search" />
        </div>
      </form>

      <h3>Recent Posts</h3>
      <ul>
        
          <li><a href="/2011/02/16/our-pain-points-with-ec2/">Our pain points with EC2 and how our moved solved them</a></li>
        
          <li><a href="/2011/02/11/how-we-did-a-datacenter-migration-with-no-downtime/">How we did a datacenter migration with no downtime</a></li>
        
          <li><a href="/2011/02/07/how-a-deprecated-method-can-take-down-a-process/">How a deprecated method can take down a process</a></li>
        
          <li><a href="/2011/01/19/love-your-dependency-management/">Love your dependency management</a></li>
        
          <li><a href="/2010/12/08/progress-through-iterating/">Progress through iterating</a></li>
        
    	</ul>
    </div>

  </div>

  <div id="footer">
    <p>The Journalist template by <a href="http://lucianmarin.com/" rel="designer">Lucian E. Marin</a></p>
  </div>
</body>
</html>